{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0f3b95-540d-404e-b005-f5bf9802b012",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d49a1-abd4-465a-a179-78b7ecb57ca0",
   "metadata": {},
   "source": [
    "Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01278288-84c6-4484-b69a-64c485e5ae97",
   "metadata": {},
   "source": [
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "- Why can't LLM spell words? Tokenization.\n",
    "- Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.\n",
    "- Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.\n",
    "- Why is LLM bad at simple arithmetic? Tokenization.\n",
    "- Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.\n",
    "- Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? Tokenization.\n",
    "- What is this weird warning I get about a \"trailing whitespace\"? Tokenization.\n",
    "- Why the LLM break if I ask it about \"SolidGoldMagikarp\"? Tokenization.\n",
    "- Why should I prefer to use YAML over JSON with LLMs? Tokenization.\n",
    "- Why is LLM not actually end-to-end language modeling? Tokenization.\n",
    "- What is the real root of suffering? Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342268cb-4222-4b00-9a23-5a662d5c8210",
   "metadata": {},
   "source": [
    "- English langauge has significantly fewer tokens."
   ]
  },
  {
   "cell_type": "raw",
   "id": "251fa98f-013a-48e3-a4c2-7216e084b207",
   "metadata": {},
   "source": [
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "127 + 677 = 804 # the tokens can seperate out the number,llms needs to take care of it.\n",
    "1275 + 6773 = 8041 \n",
    "\n",
    "Egg.\n",
    "I have an Egg.\n",
    "egg.\n",
    "EGG.\n",
    "\n",
    "ÎßåÎÇòÏÑú Î∞òÍ∞ÄÏõåÏöî. Ï†ÄÎäî OpenAIÏóêÏÑú Í∞úÎ∞úÌïú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïù∏ ChatGPTÏûÖÎãàÎã§. Í∂ÅÍ∏àÌïú Í≤ÉÏù¥ ÏûàÏúºÏãúÎ©¥ Î¨¥ÏóáÏù¥Îì† Î¨ºÏñ¥Î≥¥ÏÑ∏Ïöî.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0: # space is being tokenized and it is wasteful.The context length runs out.\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a22ffc7b-a3a3-4280-b6be-1e338715b174",
   "metadata": {},
   "source": [
    "Textual data in Python is handled with str objects, or strings. Strings are immutable sequences of Unicode code points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c284cb7-0be9-4879-9e50-3c7bd8c75439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea58bb6d-68d0-45b4-b97b-e6121461c7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\") # unicode code point for h, it only takes a single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64f36bc8-8606-499e-a9e2-4dd001444fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a624175-5bde-4d3a-8935-4512aced2aa8",
   "metadata": {},
   "source": [
    "Why can't we use simple use this in place of tokenization?\n",
    "- Vocabulary will to too large\n",
    "- Standard keeps changing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d021c4-a695-4540-acef-4ebfb8a881af",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91bd8485-89c9-4737-a23a-0520cc5dcf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ÏïàÎÖïÌïòÏÑ∏Ïöî üëã (hello in Korean!)\".encode(\"utf-8\")) # raw bytes according to utf-8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c0da533-7f8b-4453-a865-8e1184ef0905",
   "metadata": {},
   "source": [
    "1. Vocabulary Size (256 Tokens):\n",
    "UTF-8 encoding maps every character to a byte, and since a byte can represent 256 different values, the vocabulary size is fixed at 256 tokens. This means your model will have a very limited set of tokens, each corresponding to a byte. This is much smaller than using subword or word-level tokenization methods like Byte Pair Encoding (BPE) or WordPiece, where vocabulary sizes typically range from thousands to tens of thousands of tokens. While the vocabulary is compact, it results in a larger sequence length for the input text.\n",
    "\n",
    "2. Longer Sequences:\n",
    "Since you are encoding text at the byte level, a single word (which might normally be tokenized into just a few subword tokens) would now be split into multiple bytes. For example, \"hello\" in a subword tokenization might be tokenized as [hello], but with byte-level encoding, it becomes [104, 101, 108, 108, 111], where each number corresponds to the ASCII/UTF-8 code of each character.\n",
    "\n",
    "Effect: This leads to much longer sequences of tokens to represent the same text. If your model has a fixed context length (let's say 512 tokens), the model can now only \"see\" a much shorter span of the actual text compared to using larger tokens. This means less efficient use of the model's context window, as more tokens are used just to represent simple words or characters.\n",
    "3. Tiny Embedding and Prediction Layers:\n",
    "Embedding Layer: Since the vocabulary size is small (only 256 tokens), your embedding table will be very small compared to a model with a larger vocabulary. This might seem like an advantage because it's computationally cheaper, but the downside is that the model's representation capacity is reduced. Each token represents much less meaningful information compared to larger subword or word tokens.\n",
    "\n",
    "Prediction Layer: Similarly, the final layer of the model that predicts the next token will also be small because it only needs to predict 256 possible tokens. However, this limits the model's expressiveness and the ability to learn richer representations of language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9e5c8-b059-434a-aafb-0afeb76f7d72",
   "metadata": {},
   "source": [
    "### Byte pair encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "708654d6-9c70-4607-afa8-fdc6076cad52",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) is a tokenization technique widely used in natural language processing (NLP) models, especially in transformer-based models like GPT, BERT, and others. It helps balance the trade-off between vocabulary size and sequence length, and is particularly useful for reducing inefficiencies associated with both character-level and word-level tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9902cd3-e69e-4b5a-85e5-67522ba7aa12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
