{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1adb1d52-e937-4b2e-82b4-c1708a813c78",
   "metadata": {},
   "source": [
    "### Ex- 1 Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fba4043-abc4-4121-adcd-811987a05c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215d9765-f447-499c-8a88-5dd5addcc1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49235a32-8125-4ba7-96ea-eaf40d827df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14fe18b3-aa2f-420d-8016-d52291a1e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bb854e1-5170-4c83-a6dd-a76ec4300805",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list((set(''.join(words)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fae7a41-5d97-4e0f-bc13-a2896791d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0;\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b927c987-79ca-4ecb-9a35-65e9e2eb8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] + list(w)+['.']\n",
    "    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1,ix2,ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62b37e1b-8cf9-41e4-8df2-fbf1de31d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+2).float()\n",
    "P /= P.sum(2,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ff63813-306c-4f8e-93e7-2cd44c378ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "ilyasid.\n",
      "prelay.\n",
      "ocin.\n",
      "fairritoper.\n",
      "sathen.\n",
      "dannaaryanileniassibduinrwin.\n",
      "lessiyanayla.\n",
      "te.\n",
      "farmumthyfortumj.\n",
      "ponn.\n",
      "zena.\n",
      "jaylicore.\n",
      "ya.\n",
      "zoffra.\n",
      "jamilyn.\n",
      "fmouis.\n",
      "yah.\n",
      "wanaasnhavi.\n",
      "honszxhddion.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):  # Generate 20 sequences\n",
    "    out = []  # Store the output sequence\n",
    "    ix, jx = 0, 0  # Start with some initial characters, e.g., '.' as a start token if itos[0] = '.'\n",
    "    \n",
    "    while True:\n",
    "        # Get the probability distribution for the next character based on the previous two (ix, jx)\n",
    "        p = P[ix][jx]  # This gives the probabilities for the third character given ix and jx\n",
    "        # Sample the next character based on the current distribution\n",
    "        next_char_idx = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        \n",
    "        # Add the next character to the output\n",
    "        out.append(itos[next_char_idx])\n",
    "\n",
    "        # If the next character is the \"end\" character (e.g., represented by index 0), stop the sequence\n",
    "        if next_char_idx == 0:\n",
    "            break\n",
    "        \n",
    "        # Update the indices for the next iteration:\n",
    "        ix, jx = jx, next_char_idx  # Shift to the next two-character context\n",
    "    \n",
    "    # Print the generated sequence\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae5459dc-296c-44e4-bfdd-6433ad325c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood=tensor(-410414.9688)\n",
      "nll=tensor(410414.9688)\n",
      "nll/n=tensor(2.0927)\n"
     ]
    }
   ],
   "source": [
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        prob = P[ix1,ix2,ix3]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        # print(f'{ch1}{ch2} : {prob:.4f} {logprob: .4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aa937e-124b-49f8-a30a-0dbf154a449e",
   "metadata": {},
   "source": [
    "# using neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60b24d89-9015-45ac-9788-ce5e1b82d94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uninitialized_tensor = torch.empty(3, 2)\n",
    "uninitialized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03897afa-9a3b-4a80-88de-c4c179c145b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the training set of trigrams (x, y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # The index for the first character\n",
    "        ix2 = stoi[ch2]  # The index for the second character\n",
    "        ix3 = stoi[ch3]  # The index for the third character\n",
    "  # For debugging: Print the trigrams\n",
    "        xs.append((ix1, ix2))  # Append the bigram (ix1, ix2) as input\n",
    "        ys.append(ix3)  # Append the third character as output\n",
    "    \n",
    "# Convert lists to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0886ca77-3b27-4f1f-9461-e54df19278ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.6169\n",
      "10: 2.6165\n",
      "20: 2.4743\n",
      "30: 2.4242\n",
      "40: 2.4002\n",
      "50: 2.3870\n",
      "60: 2.3790\n",
      "70: 2.3739\n",
      "80: 2.3707\n",
      "90: 2.3685\n",
      "100: 2.3670\n",
      "110: 2.3660\n",
      "120: 2.3654\n",
      "130: 2.3649\n",
      "140: 2.3646\n",
      "150: 2.3643\n",
      "160: 2.3642\n",
      "170: 2.3640\n",
      "180: 2.3640\n",
      "190: 2.3639\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef10825e-2661-4e7a-8244-35e68e0e16e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odigdary.\n",
      "bramilirpmiy.\n",
      "amanstretsoladels.\n",
      "on.\n",
      "amua.\n",
      "ymcekeynasin.\n",
      "mmin.\n",
      "ea.\n",
      "ia.\n",
      "hamqkelen.\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1, ix2 = 0, 0\n",
    "    while True:\n",
    "        # previosly we used P[ix]\n",
    "        # p = P[ix]\n",
    "\n",
    "        # now we use the softmax of the logits\n",
    "        xenc = F.one_hot(torch.tensor([ix1, ix2]).to(device), num_classes = 27).float().to(device)\n",
    "        xenc = xenc.view(-1, 27*2)\n",
    "        \n",
    "        logits = xenc @ W\n",
    "        counts = torch.exp(logits)\n",
    "        p = counts / counts.sum(dim = 1, keepdim = True)\n",
    "\n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p.to(device), num_samples = 1 , replacement = True).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "\n",
    "    names.append(\"\".join(out))\n",
    "    \n",
    "for name in names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddcd77-21bc-4d86-bc2b-44bdbae9c583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b26ad8a-4e6f-421e-9c98-6d05b8de7eae",
   "metadata": {},
   "source": [
    "### EX-2 E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2afa4b32-1890-4705-8335-3fa188e2b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # The index for the first character\n",
    "        ix2 = stoi[ch2]  # The index for the second character\n",
    "        ix3 = stoi[ch3]  # The index for the third character\n",
    "  # For debugging: Print the trigrams\n",
    "        xs.append((ix1, ix2))  # Append the bigram (ix1, ix2) as input\n",
    "        ys.append(ix3)  # Append the third character as output\n",
    "    \n",
    "# Convert lists to tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(xs, ys, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b393fb2-0b3c-4926-8be0-f27f54ab781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26e371b7-5e49-4bb5-8da0-f34179d165b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d698cbb5-58d1-41e4-95de-13ccbd375550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19611, 2])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7b80c5a7-359a-4a8f-9c16-1cfd5da5225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19612, 2])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "84f55029-1cb6-48e2-b973-78f67a28913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.5938\n",
      "10: 2.5890\n",
      "20: 2.4609\n",
      "30: 2.4168\n",
      "40: 2.3957\n",
      "50: 2.3841\n",
      "60: 2.3773\n",
      "70: 2.3730\n",
      "80: 2.3703\n",
      "90: 2.3685\n",
      "100: 2.3673\n",
      "110: 2.3665\n",
      "120: 2.3659\n",
      "130: 2.3655\n",
      "140: 2.3652\n",
      "150: 2.3650\n",
      "160: 2.3649\n",
      "170: 2.3648\n",
      "180: 2.3647\n",
      "190: 2.3647\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(X_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fd7d4aa5-6aa7-4b6d-a1ff-2441fab61c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_loss(x, y, W):\n",
    "    xenc = F.one_hot(x, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "\n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "\n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(x)), y].log().mean()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e51f51b4-c6fd-4cc3-bddb-a40ca683b5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2876\n",
      "Dev Loss: 2.2881\n",
      "Test Loss: 2.2830\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Loss: {MLP_loss(X_train, y_train, W):.4f}\")\n",
    "print(f\"Dev Loss: {MLP_loss(X_val, y_val, W):.4f}\")\n",
    "print(f\"Test Loss: {MLP_loss(X_test, y_test, W):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e81477-e3e9-4f59-9565-6d3e0c7701d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "865f3d62-e40e-49fc-a5ff-c2bc99ec039b",
   "metadata": {},
   "source": [
    "### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0037054d-d9ae-42c1-8da7-1172db91adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train Loss: 4.2798 | Dev Loss 4.2945\n",
      "10: Train Loss: 2.4828 | Dev Loss 2.4814\n",
      "20: Train Loss: 2.3714 | Dev Loss 2.3704\n",
      "30: Train Loss: 2.3285 | Dev Loss 2.3279\n",
      "40: Train Loss: 2.3056 | Dev Loss 2.3054\n",
      "50: Train Loss: 2.2916 | Dev Loss 2.2919\n",
      "60: Train Loss: 2.2822 | Dev Loss 2.2829\n",
      "70: Train Loss: 2.2755 | Dev Loss 2.2766\n",
      "80: Train Loss: 2.2705 | Dev Loss 2.2720\n",
      "90: Train Loss: 2.2666 | Dev Loss 2.2684\n",
      "100: Train Loss: 2.2635 | Dev Loss 2.2656\n",
      "110: Train Loss: 2.2610 | Dev Loss 2.2633\n",
      "120: Train Loss: 2.2589 | Dev Loss 2.2614\n",
      "130: Train Loss: 2.2571 | Dev Loss 2.2598\n",
      "140: Train Loss: 2.2555 | Dev Loss 2.2584\n",
      "150: Train Loss: 2.2542 | Dev Loss 2.2572\n",
      "160: Train Loss: 2.2530 | Dev Loss 2.2561\n",
      "170: Train Loss: 2.2519 | Dev Loss 2.2552\n",
      "180: Train Loss: 2.2510 | Dev Loss 2.2543\n",
      "190: Train Loss: 2.2502 | Dev Loss 2.2536\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(X_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    # loss += 0.05 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: Train Loss: {loss.item():.4f} | Dev Loss {MLP_loss(X_val, y_val, W):.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dab980-5bb5-45d3-9b45-7e26dd3b14ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96ebcdfd-d854-42b6-987e-a853d550f05a",
   "metadata": {},
   "source": [
    "### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "897602d4-7405-4d8c-991d-dada01e0ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 15.6788\n",
      "10: nan\n",
      "20: nan\n",
      "30: nan\n",
      "40: nan\n",
      "50: nan\n",
      "60: nan\n",
      "70: nan\n",
      "80: nan\n",
      "90: nan\n",
      "100: nan\n",
      "110: nan\n",
      "120: nan\n",
      "130: nan\n",
      "140: nan\n",
      "150: nan\n",
      "160: nan\n",
      "170: nan\n",
      "180: nan\n",
      "190: nan\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    counts = torch.exp(logits)\n",
    "    probs = counts / counts.sum(dim = 1, keepdim = True)\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = - probs[torch.arange(len(X_train)), y_train].log().mean()\n",
    "    # add regularization\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def6646-6f7b-4f04-8af5-826247fb46e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57d58e79-b12a-4a30-ae67-ffd06a9672ae",
   "metadata": {},
   "source": [
    "### Ex-04 look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "46f0b530-3d41-4de7-a88d-d28cff14c6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 4.4856\n",
      "10: 2.5856\n",
      "20: 2.4639\n",
      "30: 2.4197\n",
      "40: 2.3979\n",
      "50: 2.3857\n",
      "60: 2.3784\n",
      "70: 2.3738\n",
      "80: 2.3708\n",
      "90: 2.3689\n",
      "100: 2.3675\n",
      "110: 2.3666\n",
      "120: 2.3660\n",
      "130: 2.3656\n",
      "140: 2.3653\n",
      "150: 2.3651\n",
      "160: 2.3649\n",
      "170: 2.3648\n",
      "180: 2.3647\n",
      "190: 2.3647\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27*2,27), requires_grad = True, device = device)\n",
    "for k in range(200):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(X_train, num_classes = 27).float().to(device)\n",
    "    xenc = xenc.view(-1, 27*2)\n",
    "    \n",
    "    # probs is softmax\n",
    "    logits = xenc @ W\n",
    "    \n",
    "    # loss (normalized negative log likelihood)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y_train)\n",
    "    loss += 0.2 * W.pow(2).mean()\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"{k}: {loss.item():.4f}\")\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        W -= 50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622e8e3-839a-4030-8438-801a5d09395b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a7ebd-328f-455a-8bb4-9028dbb6d0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2517e-be22-4aeb-a2bd-f3e489545db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bbd64-ef4d-4c85-b557-7f777838a465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac92e-d7f7-4eee-8542-87e713d7665e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c8b02-7d90-472f-a175-aa7beb09ca12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a072146-cc7a-4bd0-9e95-5090aa9d10e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91835000-180d-4466-9863-45bff124b498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c596bf4a-ca8c-42e8-8dc7-fdbea4d100c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b5f77-0590-4a74-8955-89590576dbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39d0e2-9c65-4a0f-9f75-101be5005b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
